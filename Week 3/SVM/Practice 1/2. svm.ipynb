{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài toán phân loại sử dụng SVM \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu: \n",
    "\n",
    "- Xây dựng được mô hình svm sử dụng thư viện sklearn. \n",
    "- Ứng dụng, hiểu cách áp dụng mô hình svm vào giải quyết bài toán thực tế (vd: phân loại tin tức dạng văn bản) .\n",
    "- Sử dụng độ đo Accuracy để làm độ đo đánh giá chất lượng mô hình. \n",
    "\n",
    "Vấn đề: \n",
    "\n",
    "- Có một tập các văn bản dạng text không có nhãn, làm sao để biết văn bản này là thuộc về thể loại nào, pháp luật, đời sống, văn học, thể thao ...\n",
    "- Cần xây dựng mô hình học máy phân loại các thể loại của văn bản dựa trên nội dung.  \n",
    "\n",
    "Dữ liệu: \n",
    "\n",
    "- Có tập các văn bản và nhãn tương ứng của từng văn bản trong một khoảng thời gian \n",
    "- Tập các nhãn - 10 nhãn văn bản: \n",
    "    > Giải trí, Khoa học - Công nghệ, Kinh tế, Pháp luật, Sức khỏe, Thể thao, Thời sự, Tin khác, Độc giả, Đời sống - Xã hội\n",
    "- Ví dụ văn bản nhãn **thể thao**: \n",
    "    > \"Dân_trí Real Madrid đã dẫn trước trong cả trận đấu , nhưng họ vẫn phải chấp_nhận bị Dortmund cầm hòa 2-2 ở Bernabeu . Real Madrid chấp_nhận đứng thứ_hai ở bảng F Champions League ...\"\n",
    "\n",
    "Bài toán: \n",
    "\n",
    "- Input: tập các từ trong văn bản 1 mẫu dữ liệu $X = [x_1, x_2, ... x_n]$\n",
    "- Output: nhãn $y$ là 1 trong 10 nhãn trên "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt thư viện xử lý ngôn ngữ cho tiếng Việt!\n",
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sử dụng sklearn.datasets.load_files để load dữ liệu từ thư mục đã down từ trước\n",
    "\n",
    "Cấu trúc thư mục như sau \n",
    "\n",
    "- data/news_1135/\n",
    "\n",
    "    - Kinh tế: \n",
    "        - bài báo 1.txt \n",
    "        - bài báo 2.txt \n",
    "    - Pháp luật\n",
    "        - bài báo 3.txt \n",
    "        - bài báo 4.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/news_1135/Tin khác/0218e1df21ce358b9c6485176a48f1fcaeedef67.txt'\n",
      " 'data/news_1135/Khoa học - Công nghệ/bf9889f5f2ffd6c92fa877d35ef0ef5f34f0666d.txt'\n",
      " 'data/news_1135/Tin khác/d74aab054ffe9f8661df13bc52b438b48a63fe48.txt'\n",
      " ...\n",
      " 'data/news_1135/Thời sự/a06c1ec4c146d3b4eb5070a1967e10e5e21bdc5b.txt'\n",
      " 'data/news_1135/Sức khỏe/4187c4a1d528fd9ea4630d2709229df0b0d09c3d.txt'\n",
      " 'data/news_1135/Thể thao/7adaf0c561796f2411340150f18417543ad4403c.txt']\n",
      "\n",
      "Tong so file: 1135\n"
     ]
    }
   ],
   "source": [
    "data_train = load_files(container_path=\"data/news_1135/\", encoding=\"utf-8\")\n",
    "\n",
    "print(data_train.filenames)\n",
    "print()\n",
    "\n",
    "print(\"Tong so file: {}\" .format( len(data_train.filenames)))\n",
    "print(\"Danh sách nhãn và id tương ứng: \", [(idx, name) for idx, name in enumerate(data_train.target_names)] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tiền xử lý dữ liệu đưa dữ liệu từ dạng text về dạng ma trận \n",
    "\n",
    "- Thử nghiệm để kiểm tra hoạt động chuyển hoá dữ liệu về dạng ma trận "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danh sách 10 từ dừng đầu tiên (từ không mang ý nghĩa phân loại):  ['a_lô', 'a_ha', 'ai', 'ai_ai', 'ai_nấy', 'ai_đó', 'alô', 'amen', 'anh', 'anh_ấy']\n",
      "10 từ đầu tiên trong từ điển:\n",
      "1 :  ('dân_trí', 6928)\n",
      "2 :  ('sở', 17869)\n",
      "3 :  ('gd', 7729)\n",
      "4 :  ('đt', 23214)\n",
      "5 :  ('tỉnh', 20851)\n",
      "6 :  ('gia_lai', 7816)\n",
      "7 :  ('văn_bản', 21779)\n",
      "8 :  ('2258', 858)\n",
      "9 :  ('sgdđt', 17039)\n",
      "10 :  ('vp', 21572)\n",
      "11 :  ('chấn_chỉnh', 4971)\n"
     ]
    }
   ],
   "source": [
    "# load dữ liệu các stopwords \n",
    "with open(\"data/vietnamese-stopwords.txt\",encoding=\"utf-8\") as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip().replace(\" \", \"_\") for x in stopwords] \n",
    "print(\"Danh sách 10 từ dừng đầu tiên (từ không mang ý nghĩa phân loại): \", stopwords[:10])\n",
    "\n",
    "# Transforming data \n",
    "# Chuyển hoá dữ liệu text về dạng vector tfidf \n",
    "#     - loại bỏ từ dừng\n",
    "#     - sinh từ điển\n",
    "module_count_vector = CountVectorizer(stop_words=stopwords)\n",
    "model_rf_preprocess = Pipeline([('vect', module_count_vector),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ])\n",
    "# Hàm thực hiện chuyển đổi dữ liệu text thành dữ liệu số dạng ma trận \n",
    "# Input: Dữ liệu 2 chiều dạng numpy.array, mảng nhãn id dạng numpy.array \n",
    "data_preprocessed = model_rf_preprocess.fit_transform(data_train.data, data_train.target)\n",
    "print(\"10 từ đầu tiên trong từ điển:\")\n",
    "i = 0\n",
    "for k,v in module_count_vector.vocabulary_.items():\n",
    "    i+=1\n",
    "    print(i, \": \", (k, v))\n",
    "    if i > 10:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 1: Sử dụng trực tiếp TfidfVectorizer()\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chú ý: có thể biến đổi dữ liệu về dạng Tf-Idf trực tiếp sử dụng TfidfVectorizer()\n",
    "# Bài tập: \n",
    "# - thực hiện điều đó\n",
    "# - hiển thị 10 từ trong văn bản đầu tiên có giá trị tfidf cao nhất\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chia dữ liệu làm 2 phần training và testing \n",
    "\n",
    "- Training chiếm 80 % dữ liệu \n",
    "- Testing chiếm 20 % dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu training:  (908, 24389) (908,)\n",
      "Dữ liệu testing:  (227, 24389) (227,)\n",
      "Danh sách nhãn và id tương ứng:  [(0, 'Giải trí'), (1, 'Khoa học - Công nghệ'), (2, 'Kinh tế'), (3, 'Pháp luật'), (4, 'Sức khỏe'), (5, 'Thể thao'), (6, 'Thời sự'), (7, 'Tin khác'), (8, 'Đời sống - Xã hội'), (9, 'Độc giả')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# chia dữ liệu thành 2 phần sử dụng hàm train_test_split.\n",
    "test_size = 0.2\n",
    "# cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split( data_preprocessed, data_train.target, test_size=test_size)\n",
    "\n",
    "# hiển thị một số thông tin về dữ liệu \n",
    "print(\"Dữ liệu training: \", X_train.shape, y_train.shape)\n",
    "print(\"Dữ liệu testing: \", X_test.shape, y_test.shape)\n",
    "print(\"Danh sách nhãn và id tương ứng: \", [(idx, name) for idx, name in enumerate(data_train.target_names)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training svm model \n",
    "\n",
    "Sử dụng thư viện sklearn để xây dựng mô hình\n",
    "-  Thử trước với hàm nhân phân tách là Linear, tham số C=1.0\n",
    "- `svm.SVC(kernel='linear', C=1.0)`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training ...\n",
      "- Train size = (908, 24389)\n",
      "- model - train complete\n"
     ]
    }
   ],
   "source": [
    "print(\"- Training ...\")\n",
    "print(\"- Train size = {}\".format(X_train.shape))\n",
    "model = svm.SVC(kernel='linear', C=1)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"- model - train complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing svm model \n",
    "\n",
    "Thực hiện dự đoán nhãn cho từng văn bản trong tập test \n",
    "\n",
    "Độ đo đánh giá: \n",
    "> accuracy = tổng số văn bản dự đoán đúng  / tổng số văn bản có trong tập test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Testing ...\n",
      "- Test Acc = 0.8370044052863436\n",
      "- Train Acc = 0.9933920704845814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"- Testing ...\")\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"- Test Acc = {}\".format(accuracy_score(y_test, y_test_pred)))\n",
    "print(\"- Train Acc = {}\".format(accuracy_score(y_train, y_train_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 2: thực hiện lại các bước trên với kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Thực hiện sử dụng model đã được train để infer 1 văn bản mới \n",
    "- Dữ liệu mới đến ở dạng dữ liệu thô => cần tiền xử lý dữ liệu về dạng dữ_liệu_ma_trận\n",
    "- infer sử dụng hàm model.predict(dữ_liệu_ma_trận) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Công phượng ghi_bàn cho đội_tuyển Việt_nam']\n",
      "  (0, 24149)\t0.4617859604824952\n",
      "  (0, 21498)\t0.23577234678310735\n",
      "  (0, 15553)\t0.6394232142292748\n",
      "  (0, 7777)\t0.4617859604824952\n",
      "  (0, 5847)\t0.33023750089838017\n"
     ]
    }
   ],
   "source": [
    "# Tiền xử lý dữ liệu sử dụng module model_rf_preprocess. \n",
    "new_doc = \"Công phượng ghi bàn cho đội tuyển Việt nam\"\n",
    "# Trước hết, cần thực hiện tách từ sử dụng pyvi\n",
    "tokenized_new_doc = ViTokenizer.tokenize(new_doc)\n",
    "# Cần đưa văn bản ở dạng mảng/vector\n",
    "tokenized_new_doc = [tokenized_new_doc]\n",
    "print(tokenized_new_doc)\n",
    "# Rồi sử dụng module model_rf_preprocess\n",
    "input_data_preprocessed = model_rf_preprocess.transform(tokenized_new_doc)\n",
    "print(input_data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 3: dự đoán nhãn của văn bản trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bài tập bổ sung: \n",
    "\n",
    "### 4.1 Thử nghiệm các tham số \n",
    "\n",
    "- Các tham số với giá trị khác nhau có thể ảnh hưởng để kết quả học \n",
    "- Cần thử nghiệm kỹ lượng để đưa ra kết quả khách quan: tham số C, gamma, kernel. \n",
    "    - Chọn mô hình với bộ tham số cho kết quả tốt nhất \n",
    "- Gợi ý: \n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    - Sử dụng grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 4: Vẽ Learning curve khảo sát Acc của SVM-linear với tham số C thay đổi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "# tham khảo miền của C\n",
    "list_C = [0.001, 0.01, 0.1, 1, 5.0, 10.0, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 5: Sử dụng GridSearchCV để tìm bộ tham số tốt nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "# Có thể tham khảo giá trị các tham số như sau\n",
    "params_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "          'gamma': [0.0001, 0.001, 0.01, 0.1],\n",
    "          'kernel':['linear','rbf', 'poly'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Phân loại số viết tay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu training =  (1437, 64) (1437,)\n",
      "Dữ liệu testing =  (360, 64) (360,)\n",
      "Training....\n",
      "Testing\n",
      "0.9916666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 4 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "target = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=test_size)\n",
    "\n",
    "print(\"Dữ liệu training = \", X_train.shape, y_train.shape)\n",
    "print(\"Dữ liệu testing = \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 6: phân loại với dữ liệu trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### exercise #####\n",
    "# Yêu cầu: Ứng dụng mô hình svm vào bài toán phân loại ảnh \n",
    "# Gợi ý: dữ liệu đã được chia train, test, Áp dụng phần 2. và 3. để training và testing model. Chú ý nên có thêm phần tuning model\n",
    "######################\n",
    "\n",
    "# model = None \n",
    "\n",
    "######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}